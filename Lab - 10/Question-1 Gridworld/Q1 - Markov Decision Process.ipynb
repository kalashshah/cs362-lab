{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYcB3e4zBFfi"
      },
      "outputs": [],
      "source": [
        "# Problem Statement 1\n",
        "\n",
        "wall = [(1,1)]\n",
        "terminal_states = ((1,3),(2,3))\n",
        "\n",
        "possible_actions = ['L','R','U','D']\n",
        "\n",
        "# non - deterministic action (equally probable)\n",
        "action_probability = {'L':0.25,'R':0.25,'U':0.25,'D':0.25}\n",
        "\n",
        "# environment action corresponding to Agent if it does not follow the desired direction (i.e follow perpendicular direction to desired one)\n",
        "environment_left = {'L':'D','R':'U','U':'L','D':'R'}\n",
        "environment_right = {'L':'U','R':'D','U':'R','D':'L'}\n",
        "\n",
        "#check validity of the cell\n",
        "def is_valid(i,j):\n",
        "    return (i,j) not in wall and i >= 0 and i < 3 and j >= 0 and j < 4\n",
        "\n",
        "#print matrix after convergence \n",
        "def print_values(V):\n",
        "  for i in range(2,-1,-1):\n",
        "    print(\" \")\n",
        "    for j in range(4):\n",
        "      v = V[i][j]\n",
        "      print(\" %.2f|\" % v, end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "#take action\n",
        "def transition(action,i,j):\n",
        "    if action == 'L':\n",
        "        new_state = (i,j-1)\n",
        "    elif action == 'R':\n",
        "        new_state = (i,j+1)\n",
        "    elif action == 'U':\n",
        "        new_state = (i+1,j)\n",
        "    else:\n",
        "        new_state = (i-1,j)   \n",
        "    return new_state\n",
        "\n",
        "def value_function(i,j,reward,reward_matrix,discount_factor=1):\n",
        "    value = 0\n",
        "    for action in possible_actions:\n",
        "        # desired action with 0.8 probability\n",
        "        state_x,state_y = transition(action,i,j)\n",
        "        if is_valid(state_x,state_y):\n",
        "            desired_action_value = (reward_matrix[state_x][state_y] + discount_factor*V_pie[state_x][state_y])\n",
        "        else:\n",
        "            desired_action_value = (reward_matrix[i][j] + discount_factor*V_pie[i][j])\n",
        "        \n",
        "        # environment action with 0.1 probability\n",
        "        state_x,state_y = transition(environment_left[action],i,j)\n",
        "        if is_valid(state_x,state_y):\n",
        "            env_action_left_value = (reward_matrix[state_x][state_y] + discount_factor*V_pie[state_x][state_y])\n",
        "        else:\n",
        "            env_action_left_value = (reward_matrix[i][j] + discount_factor*V_pie[i][j])\n",
        "        \n",
        "        # environment action with 0.1 probability \n",
        "        state_x,state_y = transition(environment_right[action],i,j)\n",
        "        if is_valid(state_x,state_y):\n",
        "            env_action_right_value = (reward_matrix[state_x][state_y] + discount_factor*V_pie[state_x][state_y])\n",
        "        else:\n",
        "            env_action_right_value = (reward_matrix[i][j] + discount_factor*V_pie[i][j])\n",
        "        \n",
        "        value_to_action = desired_action_value*0.8+env_action_left_value*0.1+env_action_right_value*0.1        \n",
        "\n",
        "        value += value_to_action*action_probability[action]\n",
        "\n",
        "    return value\n",
        "\n",
        "# iterative policy evaluation\n",
        "def iterative_policy_evaluation(iter,epsilon,reward,reward_matrix,V_pie):\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for i in range(3):\n",
        "            for j in range(4):\n",
        "                state = (i,j)\n",
        "                if state in terminal_states or state in wall:  # continue if encounter terminal state or wall\n",
        "                    continue\n",
        "                v = V_pie[i][j]\n",
        "                V_pie[i][j] = value_function(i,j,reward,reward_matrix)\n",
        "                delta = max(delta,abs(v-V_pie[i][j]))\n",
        "        iter += 1\n",
        "        if delta < epsilon:\n",
        "            print(f\"Number of iterations to converge = {iter}\")\n",
        "            break \n",
        "    print_values(V_pie)\n",
        "\n",
        "# initialize the reward matrix with given reward value except the terminal states\n",
        "def update_reward_matrix(reward):\n",
        "    reward_matrix = [[reward for _ in range(4)] for _ in range(3)]\n",
        "    reward_matrix[2][3] = 1\n",
        "    reward_matrix[1][3] = -1\n",
        "    return reward_matrix\n",
        "\n",
        "# initialize V_pie with all zeroes at start\n",
        "def initialize_V_pie():\n",
        "    V_pie = [[0 for _ in range(4)]for _ in range(3)]\n",
        "    return V_pie    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = [-0.04,-2,0.1,0.02,1]\n",
        "epsilon = 1e-8\n",
        "print(\"Value Functions corresponding to optimal policy\\n\")\n",
        "for reward in rewards:\n",
        "    print(f\"For r(S) : {reward}\")\n",
        "    reward_matrix = update_reward_matrix(reward)\n",
        "    V_pie = initialize_V_pie()\n",
        "    iterative_policy_evaluation(0,epsilon,reward,reward_matrix,V_pie)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZVuZE5gEKau",
        "outputId": "9ae1c482-9e81-4cc5-ee0b-920a149f37d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value Functions corresponding to optimal policy\n",
            "\n",
            "For r(S) : -0.04\n",
            "Number of iterations to converge = 312\n",
            " \n",
            " -1.23| -0.83| -0.28| 0.00|\n",
            " \n",
            " -1.47| 0.00| -0.87| 0.00|\n",
            " \n",
            " -1.55| -1.47| -1.22| -1.17|\n",
            "\n",
            "\n",
            "For r(S) : -2\n",
            "Number of iterations to converge = 384\n",
            " \n",
            " -59.71| -46.01| -24.32| 0.00|\n",
            " \n",
            " -65.41| 0.00| -21.94| 0.00|\n",
            " \n",
            " -63.10| -52.80| -34.49| -20.75|\n",
            "\n",
            "\n",
            "For r(S) : 0.1\n",
            "Number of iterations to converge = 324\n",
            " \n",
            " 2.95| 2.39| 1.44| 0.00|\n",
            " \n",
            " 3.10| 0.00| 0.63| 0.00|\n",
            " \n",
            " 2.85| 2.20| 1.15| 0.23|\n",
            "\n",
            "\n",
            "For r(S) : 0.02\n",
            "Number of iterations to converge = 284\n",
            " \n",
            " 0.56| 0.55| 0.46| 0.00|\n",
            " \n",
            " 0.49| 0.00| -0.23| 0.00|\n",
            " \n",
            " 0.34| 0.11| -0.20| -0.57|\n",
            "\n",
            "\n",
            "For r(S) : 1\n",
            "Number of iterations to converge = 370\n",
            " \n",
            " 29.80| 23.14| 12.48| 0.00|\n",
            " \n",
            " 32.46| 0.00| 10.30| 0.00|\n",
            " \n",
            " 31.11| 25.77| 16.43| 9.22|\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}